{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function xgboost.core._get_log_callback_func() -> Callable>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.core._get_log_callback_func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(xgb.callback.TrainingCallback):\n",
    "    \"\"\"`WandbCallback` automatically integrates XGBoost with wandb.\n",
    "\n",
    "    Arguments:\n",
    "        log_model: (boolean) if True save and upload the model to Weights & Biases Artifacts\n",
    "        log_feature_importance: (boolean) if True log a feature importance bar plot\n",
    "        importance_type: (str) one of {weight, gain, cover, total_gain, total_cover} for tree model. weight for linear model.\n",
    "        define_metric: (boolean) if True (default) capture model performance at the best step, instead of the last step, of training in your `wandb.summary`.\n",
    "\n",
    "    Passing `WandbCallback` to XGBoost will:\n",
    "\n",
    "    - log the booster model configuration to Weights & Biases\n",
    "    - log evaluation metrics collected by XGBoost, such as rmse, accuracy etc to Weights & Biases\n",
    "    - log training metric collected by XGBoost (if you provide training data to eval_set)\n",
    "    - log the best score and the best iteration\n",
    "    - save and upload your trained model to to Weights & Biases Artifacts (when `log_model = True`)\n",
    "    - log feature importance plot when `log_feature_importance=True` (default).\n",
    "    - Capture the best eval metric in `wandb.summary` when `define_metric=True` (default).\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        bst_params = dict(\n",
    "            objective=\"reg:squarederror\",\n",
    "            colsample_bytree=0.3,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            alpha=10,\n",
    "            n_estimators=10,\n",
    "            tree_method=\"hist\",\n",
    "        )\n",
    "\n",
    "        xg_reg = xgb.XGBRegressor(**bst_params)\n",
    "        xg_reg.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            callbacks=[WandbCallback()],\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_model: bool = False,\n",
    "        log_feature_importance: bool = True,\n",
    "        importance_type: str = \"gain\",\n",
    "        define_metric: bool = True,\n",
    "    ):\n",
    "        self.log_model: bool = log_model\n",
    "        self.log_feature_importance: bool = log_feature_importance\n",
    "        self.importance_type: str = importance_type\n",
    "        self.define_metric: bool = define_metric\n",
    "\n",
    "        if wandb.run is None:\n",
    "            raise wandb.Error(\"You must call wandb.init() before WandbCallback()\")\n",
    "\n",
    "        with wb_telemetry.context() as tel:\n",
    "            tel.feature.xgboost_wandb_callback = True\n",
    "\n",
    "    def before_training(self, model: Booster) -> Booster:\n",
    "        \"\"\"Run before training is finished.\"\"\"\n",
    "        # Update W&B config\n",
    "        config = model.save_config()\n",
    "        wandb.config.update(json.loads(config))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def after_training(self, model: Booster) -> Booster:\n",
    "        \"\"\"Run after training is finished.\"\"\"\n",
    "        # Log the booster model as artifacts\n",
    "        if self.log_model:\n",
    "            self._log_model_as_artifact(model)\n",
    "\n",
    "        # Plot feature importance\n",
    "        if self.log_feature_importance:\n",
    "            self._log_feature_importance(model)\n",
    "\n",
    "        # Log the best score and best iteration\n",
    "        if model.attr(\"best_score\") is not None:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"best_score\": float(cast(str, model.attr(\"best_score\"))),\n",
    "                    \"best_iteration\": int(cast(str, model.attr(\"best_iteration\"))),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def after_iteration(self, model: Booster, epoch: int, evals_log: dict) -> bool:\n",
    "        \"\"\"Run after each iteration. Return True when training should stop.\"\"\"\n",
    "        # Log metrics\n",
    "        for data, metric in evals_log.items():\n",
    "            for metric_name, log in metric.items():\n",
    "                if self.define_metric:\n",
    "                    self._define_metric(data, metric_name)\n",
    "                    wandb.log({f\"{data}-{metric_name}\": log[-1]}, commit=False)\n",
    "                else:\n",
    "                    wandb.log({f\"{data}-{metric_name}\": log[-1]}, commit=False)\n",
    "\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "\n",
    "        self.define_metric = False\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _log_model_as_artifact(self, model: Booster) -> None:\n",
    "        model_name = f\"{wandb.run.id}_model.json\"  # type: ignore\n",
    "        model_path = Path(wandb.run.dir) / model_name  # type: ignore\n",
    "        model.save_model(str(model_path))\n",
    "\n",
    "        model_artifact = wandb.Artifact(name=model_name, type=\"model\")\n",
    "        model_artifact.add_file(model_path)\n",
    "        wandb.log_artifact(model_artifact)\n",
    "\n",
    "    def _log_feature_importance(self, model: Booster) -> None:\n",
    "        fi = model.get_score(importance_type=self.importance_type)\n",
    "        fi_data = [[k, fi[k]] for k in fi]\n",
    "        table = wandb.Table(data=fi_data, columns=[\"Feature\", \"Importance\"])\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"Feature Importance\": wandb.plot.bar(\n",
    "                    table, \"Feature\", \"Importance\", title=\"Feature Importance\"\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _define_metric(self, data: str, metric_name: str) -> None:\n",
    "        if \"loss\" in str.lower(metric_name):\n",
    "            wandb.define_metric(f\"{data}-{metric_name}\", summary=\"min\")\n",
    "        elif str.lower(metric_name) in MINIMIZE_METRICS:\n",
    "            wandb.define_metric(f\"{data}-{metric_name}\", summary=\"min\")\n",
    "        elif str.lower(metric_name) in MAXIMIZE_METRICS:\n",
    "            wandb.define_metric(f\"{data}-{metric_name}\", summary=\"max\")\n",
    "        else:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
